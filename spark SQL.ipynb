{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "import org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\n\nimport org.apache.spark.rdd.RDD\n///////////\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\n\n\nimport org.apache.spark.sql.functions._\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 1
        }, 
        {
            "source": "//spark \nval conf: SparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"ww\")\nval sc: SparkContext = new SparkContext(conf)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "conf = org.apache.spark.SparkConf@5fa8d7e6\nsc = org.apache.spark.SparkContext@26ba072b\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "org.apache.spark.SparkContext@26ba072b"
                    }, 
                    "execution_count": 2, 
                    "metadata": {}
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "//sql\nval spark :SparkSession = SparkSession.builder().appName(\"app\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\nimport spark.implicits._ ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "spark = org.apache.spark.sql.SparkSession@7a5c24a\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "org.apache.spark.sql.SparkSession@7a5c24a"
                    }, 
                    "execution_count": 3, 
                    "metadata": {}
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "val l = List( (\"John\" , 28 , \"new york\") ,\n              (\"Andrew\" , 36 , \"california\") ,\n              (\"Clarcke\" , 22 , \"new york\") ,\n              (\"Kevin\" , 42 , \"denver\") ,\n              (\"Richard\" , 51 , \"sidney\")              )", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "l = List((John,28,new york), (Andrew,36,california), (Clarcke,22,new york), (Kevin,42,denver), (Richard,51,sidney))\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "List((John,28,new york), (Andrew,36,california), (Clarcke,22,new york), (Kevin,42,denver), (Richard,51,sidney))"
                    }, 
                    "execution_count": 4, 
                    "metadata": {}
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "// create new df\n\n//from json using read (csv using read.csv)\n// val df: DataFrame = spark.read.json(\"path\")\n\n// from collection \n//l = List ...\n// val df  = l.toDF(\"name\", \"age\" , \"city\")\n\n// from rdd \n//rdd[(string, int , string)\nval rdd1  = sc.parallelize(l) \n// val df  = rdd1.toDF() //df = [_1: string, _2: int ... 1 more field]\nval df = rdd1.toDF(\"name\", \"age\" , \"city\") // name columns\n\n//from a collection of  class  objects  : column names are automatically set to the attributes of the class ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "rdd1 = ParallelCollectionRDD[3] at parallelize at <console>:61\ndf = [name: string, age: int ... 1 more field]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[name: string, age: int ... 1 more field]"
                    }, 
                    "execution_count": 8, 
                    "metadata": {}
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": "//show \ndf.show() //show 20 first lines \n\n// df.show(n)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+----------+\n|   name|age|      city|\n+-------+---+----------+\n|   John| 28|  new york|\n| Andrew| 36|california|\n|Clarcke| 22|  new york|\n|  Kevin| 42|    denver|\n|Richard| 51|    sidney|\n+-------+---+----------+\n\n"
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": " df.printSchema // tree", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)\n |-- city: string (nullable = true)\n\n"
                }
            ], 
            "execution_count": 69
        }, 
        {
            "source": "# RUN SQL PURE QUERIES  , retirn always dfs   :  spark.sql( \" sql query \")\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// name the table  representing the df , its just a way to use select from that table \ndf.createOrReplaceTempView(\"people\")  // table is people\n\nval newyork_people = spark.sql(\"\"\"SELECT * FROM people WHERE city = \"new york\"  \"\"\")  // = or == is the same \n// \"\"\" used when we use \" \"  inside , \n//spark.sql(\"\"\"SELECT * FROM people WHERE city = \"new york\" ORDER BY age \"\"\")\n\nnewyork_people.show", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+--------+\n|   name|age|    city|\n+-------+---+--------+\n|   John| 28|new york|\n|Clarcke| 22|new york|\n+-------+---+--------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "newyork_people = [name: string, age: int ... 1 more field]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[name: string, age: int ... 1 more field]"
                    }, 
                    "execution_count": 10, 
                    "metadata": {}
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "## SPARK SQL API  :  not sql queries but kind of df.filter ... ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### df. : select , where (filter), limit , orderBy, groupBy, join   \n\n#### 3ways to specify a column :ceg with filter but valable to any operation: grouBy ...\n\n1- using $ :  \ndf.filter($age > 0)\n\n2- refering to df   \ndf.filter(df(\"age\") > 0)\n\n3- using sql string   \ndf.filter(\"age > 0\" )\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "val smalldf = df.filter($\"age\" > 0)\nsmalldf.show\n\n// df.where($\"age\" > 0).show\n// df.filter(df(\"age\") > 0).show\n// df.filter(\"age > 0\" ).show\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+----------+\n|   name|age|      city|\n+-------+---+----------+\n|   John| 28|  new york|\n| Andrew| 36|california|\n|Clarcke| 22|  new york|\n|  Kevin| 42|    denver|\n|Richard| 51|    sidney|\n+-------+---+----------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "smalldf = [name: string, age: int ... 1 more field]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[name: string, age: int ... 1 more field]"
                    }, 
                    "execution_count": 11, 
                    "metadata": {}
                }
            ], 
            "execution_count": 11
        }, 
        {
            "source": "// logic\n\n// df.filter($\"age\" > 0 && $\"city\" ===  \"new york\" )   or\ndf.filter(\"\"\" \"age\" > 0 && \"city\" = \"new york\" )   or", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "Name: Syntax Error.\nMessage: \nStackTrace: "
                    }, 
                    "execution_count": 12, 
                    "metadata": {}
                }
            ], 
            "execution_count": 12
        }, 
        {
            "source": "df.filter($\"city\" ===  \"new york\").show  \n\n// df.filter(df(\"city\") === \"new york\").show  \n\n// df.filter(\"\"\"city = \"new york\" \"\"\").show     // or \n// df.filter(\"city = 'new york' \").show\n\n\n// =  only with sql like strings \n// values that are strings must always be in \"\"  , so the sql string must be in \"\"\"city = \"new york\" \"\"\"\n//or \"city = 'new york' \"", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+--------+\n|   name|age|    city|\n+-------+---+--------+\n|   John| 28|new york|\n|Clarcke| 22|new york|\n+-------+---+--------+\n\n"
                }
            ], 
            "execution_count": 13
        }, 
        {
            "source": "///////   example   using sql query string to sqy which coluumn \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "val newdf = df.select(\"name\" , \"age\").where(\"city = 'new york'\").orderBy(\"age\") \n\n// or orderBy(asc/desc(\"age\"))   or  .sort($\"col1\".desc)\n// or orderBy(\"age\" , \"name\")\n\nnewdf.show", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+\n|   name|age|\n+-------+---+\n|Clarcke| 22|\n|   John| 28|\n+-------+---+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "newdf = [name: string, age: int]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[name: string, age: int]"
                    }, 
                    "execution_count": 14, 
                    "metadata": {}
                }
            ], 
            "execution_count": 14
        }, 
        {
            "source": "val dftest = sc.parallelize(Seq((4, \"blah\", 2), (2, \"\", 3), (56, \"foo\", 3), (100, null, 5)))\n    .toDF(\"A\", \"B\", \"C\")\n\nval newDf = dftest.withColumn(\"D\", when($\"B\".isNull or $\"B\" === \"\", 0).otherwise(1))\n// newDf.show", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "// df.select(\"name\" , \"age\").where(\"city = 'new york'\").orderBy(\"age\").show   //saem with\n// df.select($\"name\" , $\"age\").where(\"city = 'new york'\").orderBy(\"age\").show\ndf.select(df(\"name\") , $\"age\").where(\"city = 'new york'\").orderBy(\"age\").show", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+\n|   name|age|\n+-------+---+\n|Clarcke| 22|\n|   John| 28|\n+-------+---+\n\n"
                }
            ], 
            "execution_count": 21
        }, 
        {
            "source": "COLLECT", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "val tomaster = df.collect()\ntomaster(0)(0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "tomaster = Array([John,28,new york], [Andrew,36,california], [Clarcke,22,new york], [Kevin,42,denver], [Richard,51,sidney])\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "John"
                    }, 
                    "execution_count": 15, 
                    "metadata": {}
                }
            ], 
            "execution_count": 15
        }, 
        {
            "source": "# groupby  \nhttps://www.coursera.org/learn/scala-spark-big-data/lecture/3U9UW/dataframes-1\n\ngroupBdoesnt return a DF  , groupedDataFrame    ,  we then use  -  aggregate : sum($\"col\"))   \naggregate:  sum   , min , max , avg    .count   or agg(sum or any other , count also )\n\n or agg(sum()): if we spcify which column to aggregate  (sum,min ... ) on : df.groupBy($\"city\").agg(min($\"age\"))   // .orderBy($\"count(age)\".asc).show \n \nto get different columns with grouBy : df.groupBy(\"col1\" , \"col2\") and if necessary .count(\"col1\").orderBy($\"count(clo1)\")\n\norder with count:    orderBy($\"count(clo1).asc\")\n\n\n\nnote : df.groupBy($\"city\",$\"age\") doesnt do anything since all ages are different: fro groupby must be duplicates for city and also for age : eg:\ncity0  : age0 (group)\ncity0  : age1 (other group) \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " #### theres sum count min max ... API on groupeddataset () groupBy\n \n #### and theres agg   API: \n agg(sum())  : if we  use $\"col\"    : df.groupBy($\"city\").agg(min($\"age\"))   // .orderBy($\"count(age)\".asc).show \n #### sum, min , max, avg, mean ,  stddev , first ,last , count  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.groupBy($\"city\").count().orderBy($\"count\".asc).show //df.groupBy(\"city\").", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+-----+\n|      city|count|\n+----------+-----+\n|    denver|    1|\n|    sidney|    1|\n|california|    1|\n|  new york|    2|\n+----------+-----+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "lastException: Throwable = null\n"
                    }, 
                    "metadata": {}
                }
            ], 
            "execution_count": 31
        }, 
        {
            "source": "// df.groupBy(\"city\").count().orderBy($\"count\".asc).show\ndf.groupBy(\"city\").agg(count(\"*\") as \"counted\").orderBy($\"counted\".asc).show  // use of agregate y have to specify on which col y want to count or * to say implicit ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+-------+\n|      city|counted|\n+----------+-------+\n|    sidney|      1|\n|    denver|      1|\n|california|      1|\n|  new york|      2|\n+----------+-------+\n\n"
                }
            ], 
            "execution_count": 97
        }, 
        {
            "source": "// df.groupBy($\"city\").agg(count($\"age\")).orderBy($\"count(age)\".asc).show  \n// ####df.groupBy(\"city\").count(\"age\")     error   use count() ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "Name: Compile Error\nMessage: <console>:53: error: too many arguments for method count: ()org.apache.spark.sql.DataFrame\n       df.groupBy(\"city\").count(\"age\")\n                               ^\n\nStackTrace: "
                    }, 
                    "execution_count": 12, 
                    "metadata": {}
                }
            ], 
            "execution_count": 12
        }, 
        {
            "source": "// anotherdf.groupBy(\"zip\").max(\"col\")\n\n// df.groupBy($\"city\").avg(\"age\").show   \n\ndf.groupBy($\"city\").avg(\"age\").orderBy($\"avg(age)\".asc).show   ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+--------+\n|      city|avg(age)|\n+----------+--------+\n|  new york|    25.0|\n|california|    36.0|\n|    denver|    42.0|\n|    sidney|    51.0|\n+----------+--------+\n\n"
                }
            ], 
            "execution_count": 18
        }, 
        {
            "source": "scala", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// (-2 to -1).map(0.5 + ) is same as  +_\n\n// val f  = (x:Int , y : Char) => {println(x) ; print(y)}\n// f(-1,'p')\n\n// val f  = (x:Int) => (y : Char) => {println(x) ; print(y)}\n// f(-1)('p') // no f(-1,'p')\n\n// val zscore = (mean:R, sd:R) => (x:R) => (x-mean)/sd\n\n// val v42 = 42\n\n// Some(3) match {\n//   case Some(v42) => println(\"42\") //\u201cv42\u201d is interpreted as a name matching any Int value   use `v42`\n//   case _ => println(\"Not 42\")\n// }\n\n// class C(var x: Char) {\n//   def this() { this('u')}\n// }", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### missing data   , null or Nan   ,return new dataframe : \n\n-drop    \n-replace ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.drop() // drop row if any col in null or NAN  \n// drop(\"all\")  dropped only if all cols of the row to drop is null or nan\n//dro(Array(\"col1\",\"col2\" ))   drop row only if the cols  , col1-2 are null or nan \n\n//replace\ndf.fill(0) //numeric type columns affected \ndf.fill(Map(\"col0\" -> val0   , \"col1\" -> val1 )) // each col is different \ndf.replace(Array(\"col\"), Map(val1 , val2)   )  //replace on col , only val1 with val2  only ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Actions    \n\ncollect , cout , first show take ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nval l = df.collect()\nval u = l(0)   // Array[Row)])\nu.getClass\n\nu(0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "l = Array([John,28,new york], [Andrew,36,california], [Clarcke,22,new york], [Kevin,42,denver], [Richard,51,sidney])\nu = [John,28,new york]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "John"
                    }, 
                    "execution_count": 92, 
                    "metadata": {}
                }
            ], 
            "execution_count": 92
        }, 
        {
            "source": "df.head() // .first    return Row\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[John,28,new york]"
                    }, 
                    "execution_count": 93, 
                    "metadata": {}
                }
            ], 
            "execution_count": 93
        }, 
        {
            "source": "## JOINS", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df1.join(df2  ,   $\"df1.col\" === $\"df2.col\") // same col in the two dfs  \n\n// specify outer join\ndf1.join(df2  ,   $\"df1.col\" === $\"df2.col\" , \"right_outer\") // outer , left_outer , leftsemi\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# DATASET  \n  /   \n  /   \n  /   \n  /   \n  /....    \n    \n    \n      \n        \n\n\n\n\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// ************** from DF \n\n// df.as[(String, Int , String)].show\n\n\n// from collection of objects  in DF\ncase class Company(name: String, foundingYear: Int, numEmployees: Int) // following  in another cell , otherwise error retrn .", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "defined class Company\n"
                    }, 
                    "metadata": {}
                }
            ], 
            "execution_count": 14
        }, 
        {
            "source": "\nval inputSeq = Seq(Company(\"ABC\", 1998, 310), Company(\"XYZ\", 1983, 904), Company(\"NOP\", 2005, 83))\nval df1 = sc.parallelize(inputSeq).toDF()\n\nval companyDS = df1.as[Company]\ncompanyDS.show\n\n\n// from rdd: \n// sc.parallelize(inputSeq).toDS().show\n\n//from \n// inputSeq.toDS().show   ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----+------------+------------+\n|name|foundingYear|numEmployees|\n+----+------------+------------+\n| ABC|        1998|         310|\n| XYZ|        1983|         904|\n| NOP|        2005|          83|\n+----+------------+------------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "inputSeq = List(Company(ABC,1998,310), Company(XYZ,1983,904), Company(NOP,2005,83))\ndf1 = [name: string, foundingYear: int ... 1 more field]\ncompanyDS = [name: string, foundingYear: int ... 1 more field]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[name: string, foundingYear: int ... 1 more field]"
                    }, 
                    "execution_count": 26, 
                    "metadata": {}
                }
            ], 
            "execution_count": 26
        }, 
        {
            "source": "// // from collection \n\nl.toDS()  // default cols \n\nval cols = List(\"name\", \"age\" , \"city\") \n// toDF(colsits incorrect because toDf(\"name\", \"age\" , \"city\") but not toDF( (\"name\", \"age\" , \"city\")  )   we use :  varargs passing \n\n\nval ds = l.toDF(cols:_*).as[(String,Int,String)]     \nds.show\n\n\n// \n\n// //from rdd\n// rdd1.toDS().show", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+----------+\n|   name|age|      city|\n+-------+---+----------+\n|   John| 28|  new york|\n| Andrew| 36|california|\n|Clarcke| 22|  new york|\n|  Kevin| 42|    denver|\n|Richard| 51|    sidney|\n+-------+---+----------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "cols = List(name, age, city)\nds = [name: string, age: int ... 1 more field]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[name: string, age: int ... 1 more field]"
                    }, 
                    "execution_count": 6, 
                    "metadata": {}
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "val wordsDataset = sc.parallelize(Seq(\"Spark I am your father\"     ,       \"May the spark be with you\"   ,      \"Spark I am your father\")).toDS()\n// wordsDataset.show\n\nval groupedDataset = wordsDataset.flatMap(_.toLowerCase.split(\" \"))\n                                 .filter(_ != \"\")\n                                 .groupBy(\"value\") //the columns is called value by default \n\n\nval countsDataset = groupedDataset.count()\ncountsDataset.show()\n// countsDataset.groupBy(\"count\").agg(count(\"value\")).show\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+-----+\n| value|count|\n+------+-----+\n|father|    2|\n|   you|    1|\n|  with|    1|\n|    be|    1|\n|  your|    2|\n|   may|    1|\n| spark|    3|\n|   the|    1|\n|     i|    2|\n|    am|    2|\n+------+-----+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "wordsDataset = [value: string]\ngroupedDataset = RelationalGroupedDataset: [grouping expressions: [value: string], value: [value: string], type: GroupBy]\ncountsDataset = [value: string, count: bigint]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[value: string, count: bigint]"
                    }, 
                    "execution_count": 105, 
                    "metadata": {}
                }
            ], 
            "execution_count": 105
        }, 
        {
            "source": "## combining    Functional  rdd  and Relational sql   operations on DS", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import org.apache.spark.sql.functions._\n\nval wordsDataset = sc.parallelize(Seq(\"Spark I am your father\", \"May the spark be with you\", \"Spark I am your father\")).toDS()\nval result = wordsDataset\n              .flatMap(_.split(\" \"))               // Split on whitespace\n              .filter(_ != \"\")                     // Filter empty words\n              .map(_.toLowerCase())\n              .groupBy($\"value\")                   // Count number of occurrences of each word\n              .agg(count(\"*\") as \"numOccurances\")\n              .orderBy($\"numOccurances\" desc)      // Show most common words first\nresult.show()    //DS \n\n\n\n\n// agg(avg($\"col\".as[]Double))\n\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+-------------+\n| value|numOccurances|\n+------+-------------+\n| spark|            3|\n|  your|            2|\n|father|            2|\n|     i|            2|\n|    am|            2|\n|  with|            1|\n|    be|            1|\n|   you|            1|\n|   the|            1|\n|   may|            1|\n+------+-------------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "wordsDataset = [value: string]\nresult = [value: string, numOccurances: bigint]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "warning: there was one feature warning; re-run with -feature for details\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[value: string, numOccurances: bigint]"
                    }, 
                    "execution_count": 50, 
                    "metadata": {}
                }
            ], 
            "execution_count": 50
        }, 
        {
            "source": "y might  use functional  on DF : df.map( row => row(0).asInstanceOf[type] )   it return a dataset : type specification required asInstance Of\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "// DF \n// df.map( row => row(0).asInstanceOf[String]).show\n\n//DS\nds.map( tuple => (tuple._1 , tuple._2)).show", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+---+\n|     _1| _2|\n+-------+---+\n|   John| 28|\n| Andrew| 36|\n|Clarcke| 22|\n|  Kevin| 42|\n|Richard| 51|\n+-------+---+\n\n"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "ds.filter($\"city\".as[String] === \"thecity\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "ds.select($\"name\" , $\"city\").show\n// ds.distinct().show", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+----------+\n|   name|      city|\n+-------+----------+\n|   John|  new york|\n| Andrew|california|\n|Clarcke|  new york|\n|  Kevin|    denver|\n|Richard|    sidney|\n+-------+----------+\n\n"
                }
            ], 
            "execution_count": 86
        }, 
        {
            "source": "# DF has rows as single element (row(0) ... )  : NOT TYPED ,   DATASET has  TUPLES :  eg  ds.map(x  =>   )    x is a tuple representing elements of the same row  : TYPED ,careful or y get errors , but in DF the compiler dont detect type problems ,like sql \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "ds.map(x => x._1 + \" , age \" + x._2.toString ).show  //x is a tuple ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------+\n|           value|\n+----------------+\n|   John , age 28|\n| Andrew , age 36|\n|Clarcke , age 22|\n|  Kevin , age 42|\n|Richard , age 51|\n+----------------+\n\n"
                }
            ], 
            "execution_count": 93
        }, 
        {
            "source": "# groupby key\n\ngroupby key (l => l._1)    // l is a tuple (String, Int, String) because its DS not DF \nthe result: keyvaluegroupeddataset     must be followed by affregation  \n\nafter you can use :   \nagg (col : V) : DS{K,U}  \nreducegroups (f: (V V ) =>V) returns a DS(K,V)  , reduces elements grouped using f functin like reduce acc: V , cur : V return V  \nmapGroup( f(K, Iterator[]V => U   )  )   : Dataset[]U\nmapValues\n//  \n//  \neven using grouby KEY : the groups still contain all the columns including key ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "val dsgrouped = ds.groupByKey(l => l._1)  // l is a tuple because its DS not DF \ndsgrouped.agg(avg(\"age\").as[Double]).show   \n//ds.groupByKey(l => l._1).agg(avg(\"age\")).show    ERROR      use agg(avg(\"age\").as[Double] to avoiid ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+--------+\n|  value|avg(age)|\n+-------+--------+\n|Clarcke|    22.0|\n|   John|    28.0|\n|Richard|    51.0|\n| Andrew|    36.0|\n|  Kevin|    42.0|\n+-------+--------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "dsgrouped = KeyValueGroupedDataset: [key: [value: string], value: [_1: string, _2: int ... 1 more field(s)]]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "KeyValueGroupedDataset: [key: [value: string], value: [_1: string, _2: int ... 1 more field(s)]]"
                    }, 
                    "execution_count": 121, 
                    "metadata": {}
                }
            ], 
            "execution_count": 121
        }, 
        {
            "source": "ds.groupByKey(l => (l._1,l._3)).agg(avg(\"age\").as[Double]).show        // l is a tuple because its DS not DF ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "Name: Compile Error\nMessage: <console>:62: error: type mismatch;\n found   : String(\"avg\")\n required: org.apache.spark.sql.Encoder[?]\n       ds.groupByKey(l => (l._1,l._3)).agg(avg(\"age\").as[Double] as \"avg\").show        // l is a tuple because its DS not DF\n                                                                    ^\n\nStackTrace: "
                    }, 
                    "execution_count": 15, 
                    "metadata": {}
                }
            ], 
            "execution_count": 15
        }, 
        {
            "source": "//reduceGroups(f: (V V ) =>V) returns a DS(K,V)   : V is the tuple (String, Int, String)\n// val dsgrouped = ds.groupByKey(l => l._1)\n\ndsgrouped.reduceGroups((acc , y ) => (y._1, y._2 ,y._3) ).show // last tuple for each group     \n// (f: ((String, Int, String), (String, Int, String)) => (String, Int, String))\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+------------------------------+\n|  value|ReduceAggregator(scala.Tuple3)|\n+-------+------------------------------+\n|Clarcke|          [Clarcke, 22, new...|\n|   John|          [John, 28, new york]|\n|Richard|          [Richard, 51, sid...|\n| Andrew|          [Andrew, 36, cali...|\n|  Kevin|           [Kevin, 42, denver]|\n+-------+------------------------------+\n\n"
                }
            ], 
            "execution_count": 131
        }, 
        {
            "source": "// dsgrouped.keys.show\ndsgrouped.mapGroups((K , group) => { val tupl : (String,Int,String) = group.next ;  (tupl._1) }   ).show   //group iterator so next only \n\n//group contain the key  also , ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+\n|  value|\n+-------+\n|Clarcke|\n|   John|\n|Richard|\n| Andrew|\n|  Kevin|\n+-------+\n\n"
                }
            ], 
            "execution_count": 174
        }, 
        {
            "source": "val dstest  = List( (-1,\"kl\") , (-1,\"ku\") , (-1,\"kp\") , (-2,\"jiu\") , (-2,\"gjhj\")    ).toDS()\n// dstest.show \n//dstest.groupByKey(tupl => tupl._1).reduceGroups( (acc : (Int,String), c:(Int,String) ) =>    (acc._1 + c._1 , acc._2 + c._2)    ) // same type \n\n// the performance isnt good because foldleft is not parrallel at all \n// dstest.groupByKey(tupl => tupl._1).mapGroups((k , group) => (k , group.foldLeft(\"\")((acc :String ,p : (Int,String)) => acc + p._2)) ).orderBy($\"_1\").show\n\n\n//using reduceGroups \ndstest.groupByKey(p => p._1)\n        .mapValues(p => p._2) // delete the key in the values \n          .reduceGroups((acc : String , str) => acc + str )      // (v,v) => v\n           .show\n\n\n// not performant ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+-------+\n| _1|     _2|\n+---+-------+\n| -2|jiugjhj|\n| -1| klkukp|\n+---+-------+\n\n+-----+----------------------------------+\n|value|ReduceAggregator(java.lang.String)|\n+-----+----------------------------------+\n|   -1|                            klkukp|\n|   -2|                           jiugjhj|\n+-----+----------------------------------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "dstest = [_1: int, _2: string]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[_1: int, _2: string]"
                    }, 
                    "execution_count": 13, 
                    "metadata": {}
                }
            ], 
            "execution_count": 13
        }, 
        {
            "source": "# Aggregator  : like the rdd aggregator ( ( foldleft ) then  fold )  //parallelizable\nclass aggregator [IN , BUF , OUT]\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import org.apache.spark.sql.expressions.Aggregator\nimport org.apache.spark.sql.Encoder\n\n\nval dstest  = List( (-1,\"kl\") , (-1,\"ku\") , (-1,\"kp\") , (-2,\"jiu\") , (-2,\"gjhj\")    ).toDS()\n// how to concat all the string by key", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "dstest = [_1: int, _2: string]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[_1: int, _2: string]"
                    }, 
                    "execution_count": 21, 
                    "metadata": {}
                }
            ], 
            "execution_count": 21
        }, 
        {
            "source": "\n\nval strConcat = new Aggregator[(Int,String) , String , String]{   // [ IN ,BUF , OUT   )  ====> (Int,String) , String , String\n    import org.apache.spark.sql.Encoders\n    \n    def zero : String  = \"\"\n    def reduce(b:String , a:(Int,String) ): String  = a._2 + b\n    def merge(b1: String ,  b2: String ): String = b1 + b2\n    def finish(r:String ):  String  =     r\n    override def bufferEncoder: Encoder[String] =  Encoders.STRING //    override def bufferEncoder: Encoder[BUF] = \n    override def outputEncoder: Encoder[String] =   Encoders.STRING//   override def bufferEncoder: Encoder[OUT] = \n\n    \n}.toColumn()\n\n\ndstest.groupByKey(p => p._1)\n          .agg(strConcat.as[String])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "Name: java.lang.RuntimeException\nMessage: Unsupported literal type class scala.runtime.BoxedUnit ()\nStackTrace: java.lang.RuntimeException: Unsupported literal type class scala.runtime.BoxedUnit ()\n  at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:77)\n  at org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:163)\n  at org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:163)\n  at scala.util.Try.getOrElse(Try.scala:79)\n  at org.apache.spark.sql.catalyst.expressions.Literal$.create(literals.scala:162)\n  at org.apache.spark.sql.functions$.typedLit(functions.scala:113)\n  at org.apache.spark.sql.functions$.lit(functions.scala:96)\n  at org.apache.spark.sql.Column.apply(Column.scala:212)"
                    }, 
                    "execution_count": 33, 
                    "metadata": {}
                }
            ], 
            "execution_count": 33
        }, 
        {
            "source": "//Actions on DS\nval k = ds.collect() \nk(0)._1\n//cand do anything on the tuple \n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "k = Array((John,28,new york), (Andrew,36,california), (Clarcke,22,new york), (Kevin,42,denver), (Richard,51,sidney))\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "lastException: Throwable = null\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "John"
                    }, 
                    "execution_count": 34, 
                    "metadata": {}
                }
            ], 
            "execution_count": 34
        }, 
        {
            "source": "//on DS: collect , count , first , foreach , reduce , show , take \n\n# When to use DF DS RDD\nDS: \n- y work on structured/semi structered data\n-y want typesafety\n-y need to use FUNCTIONAL API\n-y need good performance, but not necessary the best performanc ,\n\nDF: \n- y work on structured/semi structered data\n- you want the best performance , optimized for you\n\nRDD: \n-y have unstrcutered data \n- y need to fine tune and manage low-level details od rdd computations\n-you have data types that cannot be serialized by the spark sql Encoder (not supported)\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark", 
            "name": "scala", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "version": "2.11.12", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat": 4
}